{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f32aa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dccf34-7698-4979-b366-71e774f088f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import wilcoxon\n",
    "from statsmodels.stats.weightstats import ttost_paired\n",
    "\n",
    "from dataloader import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('talk', rc={\"font.size\":18,\"axes.titlesize\":18,\"axes.labelsize\":18,\"xtick.labelsize\":18})\n",
    "sns.set_palette('colorblind')\n",
    "import dataframe_image as dfi\n",
    "from sklearn import tree\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "\n",
    "STORAGE_PATH = './results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94526744-c484-4004-a315-3948d6a91e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dir', default='alldata')  # output directory\n",
    "parser.add_argument('--dataset_id', default=-1, type=int)  # 1 indexed\n",
    "parser.add_argument('--model_id', default=-1, type=int)  # 1 indexed\n",
    "parser.add_argument('--data_max_size', default=1000000000, type=int)  # max number of data points in each dataset\n",
    "parser.add_argument('--exclude_group_feature', action='store_true')  # do not give group indicator as a feature\n",
    "parser.add_argument('--to_categorical', action='store_true')  # use categorical features\n",
    "parser.add_argument('--steps_alg3', default=10000, type=int)  # param for minimax algorithm of Abernethy et al. 2022\n",
    "parser.add_argument('--steps_minimax_fair', default=10000, type=int)  # param for minimax algorithm of Diana et al. 2021\n",
    "parser.add_argument('--error_minimax_fair', default='Log-Loss', type=str)  # Options 'MSE', '0/1 Loss', 'FP', 'FN', 'Log-Loss', 'FP-Log-Loss', 'FN-Log-Loss'\n",
    "parser.add_argument('--warm_start_minimax_fair', action='store_true')  # start from a high weight on worst-off group\n",
    "\n",
    "args = parser.parse_args('')\n",
    "print(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1eed5502-50fe-448b-889e-3c8845586c22",
   "metadata": {},
   "source": [
    "## Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d2cfaf-b461-47e0-9692-cf2d8629e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify models to plot\n",
    "MODEL_TYPES_TO_PLOT = [\n",
    "    'TabularMLPClassifier',\n",
    "    'LogisticRegressionSGD',\n",
    "    'LogisticRegressionSGDdim2',\n",
    "    'LogisticRegressionSGDdim4',\n",
    "    'LogisticRegressionSGDdim8',\n",
    "    # 'LogisticRegressionSGDdim16',\n",
    "    'DecisionTree2',\n",
    "    'DecisionTree4',\n",
    "    'DecisionTree8',\n",
    "    'RandomForest',\n",
    "    'LinearSVC',\n",
    "]\n",
    "\n",
    "# Helper functions to read saved files,\n",
    "# compute worst-case performance, and tabulate\n",
    "def findworseoff(df):\n",
    "    erm = df[df['trained_on']=='full'].value.min()\n",
    "    minmax = df[df['trained_on']=='minmax'].value.min()\n",
    "    diff_mm = (erm - minmax)\n",
    "    df['difference_minmax'] = diff_mm\n",
    "    df['difference_minmax_percentage_full'] = diff_mm/(erm+1e-5)*100\n",
    "    df['minmax'] = minmax\n",
    "    df['erm'] = erm\n",
    "    group_optimal = df[~df['trained_on'].isin(['full','minmax'])].value.min()\n",
    "    df['group_optimal'] = group_optimal\n",
    "    df['difference_group_optimal'] = (erm - group_optimal)\n",
    "    return df\n",
    "\n",
    "def prepare_dataframe_worseoff(directories):\n",
    "    files = []\n",
    "    \n",
    "    for d in directories:\n",
    "        files += glob.glob(os.path.join(d,\"*/metrics_did*_dataset.csv\"))\n",
    "\n",
    "    print(f'total results files {len(files)}')\n",
    "    print(files)\n",
    "    \n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        dfs.append(pd.read_csv(f))\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    df['group_type'] = df['group_type'].fillna(value='custom')\n",
    "    \n",
    "    # Example of one dataset\n",
    "    df_one_dataset = df[(df['dataset']=='adult_income_NY')\n",
    "                    & (df['model_type']=='RandomForest')\n",
    "                    & (df['eval_data_type']=='train')\n",
    "                    & (df['metric'].isin(['accuracy']))\n",
    "    ]\n",
    "    df_one_dataset_pivot = df_one_dataset.pivot_table(index=['dataset','model_type','group_type','eval_data_type','trained_on','metric'], columns=['evaluated_on'], values=['value'])\n",
    "    print(df_one_dataset_pivot)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    df_diff = df.groupby(['dataset','group_type','metric','model_type','eval_data_type']).apply(findworseoff)\n",
    "    \n",
    "    df_full = df_diff[df_diff['trained_on']=='full']\n",
    "    \n",
    "    df_worse_performing_group = df_full.drop(['evaluated_on','value'], axis=1).drop_duplicates()  # lower values of metrics are worse\n",
    "    \n",
    "    return df_worse_performing_group"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92b5bd32-4084-459f-b855-86924c3a6fd7",
   "metadata": {},
   "source": [
    "## Comparing ERM w minimax-fair and group-optimal models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9096be1-72a5-4597-aa5f-d3cd80b7f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the results directory\n",
    "directories = [\n",
    "    '../results/allmodel',\n",
    "]\n",
    "\n",
    "files = []\n",
    "    \n",
    "for d in directories:\n",
    "    files += glob.glob(os.path.join(d,\"*/metrics_did*_dataset.csv\"))\n",
    "\n",
    "print(len(files))\n",
    "\n",
    "dfs = []\n",
    "for f in files:\n",
    "    dfs.append(pd.read_csv(f))\n",
    "df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9939d07a-1422-476a-9894-590555eba06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_worseoff = prepare_dataframe_worseoff(directories)\n",
    "df_worseoff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "546b3d27-5efb-40ca-80ab-b7c23a8da6ee",
   "metadata": {},
   "source": [
    "## Worst-case accuracy plot to make Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8451986-b1e8-4004-aa9f-29a2f18fe2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_worseoff[\n",
    "    (df_worseoff['metric']=='accuracy') &\n",
    "    (df_worseoff['eval_data_type']=='train') # change to 'test' for Figure 3\n",
    "]\n",
    "df_plot.sort_values(['dataset','group_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1763097-7e9c-4e27-8326-3a7412df0877",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot_ = df_plot[df_plot['model_type'].isin(['TabularMLPClassifier', 'LogisticRegressionSGD',\n",
    "       'DecisionTree8', 'RandomForest', 'LinearSVC'])]\n",
    "\n",
    "df_plot_.loc[:, 'model_type'] = df_plot_.loc[:, 'model_type'].str.replace('TabularMLPClassifier', 'MLP')\n",
    "df_plot_.loc[:, 'model_type'] = df_plot_.loc[:, 'model_type'].str.replace('LogisticRegressionSGD', 'Logistic Regression')\n",
    "df_plot_.loc[:, 'model_type'] = df_plot_.loc[:, 'model_type'].str.replace('DecisionTree8', 'Decision Tree depth 8')\n",
    "df_plot_.loc[:, 'model_type'] = df_plot_.loc[:, 'model_type'].str.replace('RandomForest', 'Random Forest')\n",
    "df_plot_.loc[:, 'model_type'] = df_plot_.loc[:, 'model_type'].str.replace('LinearSVC', 'Linear SVC')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2,figsize=(10,18))\n",
    "g1 = sns.scatterplot(data=df_plot_,\n",
    "            x='erm', y='group_optimal',\n",
    "            hue='model_type', style='model_type', s=250, alpha=0.7,\n",
    "            ax=ax1)\n",
    "ax1.set_xlabel('ERM,\\n worst-off group accuracy', fontsize=22)\n",
    "ax1.set_ylabel('GROUP-OPTIMAL,\\n worst-off group accuracy', fontsize=22)\n",
    "xpoints = ypoints = ax1.get_xlim()\n",
    "ax1.plot(xpoints, ypoints, linestyle='--', color='k', lw=1, scalex=False, scaley=False)\n",
    "ax1.get_legend().remove()\n",
    "\n",
    "g2 = sns.scatterplot(data=df_plot_,\n",
    "            x='erm', y='minmax',\n",
    "            hue='model_type', style='model_type', s=250, alpha=0.7,\n",
    "            ax=ax2)\n",
    "ax2.set_xlabel('ERM,\\n worst-off group accuracy', fontsize=22)\n",
    "ax2.set_ylabel('MINIMAX,\\n worst-off group accuracy', fontsize=22)\n",
    "ax2.plot(xpoints, ypoints, linestyle='--', color='k', lw=1, scalex=False, scaley=False)\n",
    "ax2.legend(loc='upper center', bbox_to_anchor=(0.5, -0.25), ncol=3)\n",
    "fig.tight_layout(pad=2.0)\n",
    "plt.savefig('accuracy_train.pdf', bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cd20a40-13ad-46fe-a25a-033f6830d339",
   "metadata": {},
   "source": [
    "## Testing for equivalence and non-inferiority hypotheses to make Tables 2-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a36813c-cde0-4738-b260-3da2ea5933b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lower_threshold, upper_threshold = -0.01, 0.01\n",
    "df_pvalues = pd.DataFrame(\n",
    "    df_worseoff.groupby(['metric','model_type','eval_data_type']).apply(\n",
    "        lambda x: pd.Series({\n",
    "            \"pvalue_equivalence_minmax\": ttost_paired(x[\"erm\"], x[\"minmax\"], low=lower_threshold, upp=upper_threshold)[0],\n",
    "            \"pvalue_noninferior_minmax\": ttost_paired(x[\"erm\"], x[\"minmax\"], low=lower_threshold, upp=upper_threshold)[1][1],\n",
    "            \"pvalue_equivalence_group_optimal\": ttost_paired(x[\"erm\"], x[\"group_optimal\"], low=lower_threshold, upp=upper_threshold)[0],\n",
    "            \"pvalue_noninferior_group_optimal\": ttost_paired(x[\"erm\"], x[\"group_optimal\"], low=lower_threshold, upp=upper_threshold)[1][1]\n",
    "        })\n",
    "    )\n",
    ").reset_index()\n",
    "\n",
    "for metric in df_pvalues['metric'].unique():\n",
    "    if metric in ['neglogloss', 'accuracy']:\n",
    "        for eval_data_type in df_pvalues['eval_data_type'].unique():\n",
    "            print(metric, eval_data_type)\n",
    "            tab = df_pvalues[\n",
    "                (df_pvalues['metric']==metric)\\\n",
    "                & (df_pvalues['eval_data_type']==eval_data_type)\\\n",
    "            ][['model_type','pvalue_equivalence_group_optimal','pvalue_noninferior_group_optimal','pvalue_equivalence_minmax','pvalue_noninferior_minmax']]\n",
    "            print(tab.round(decimals=4).to_latex(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
